{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55231714",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5262fa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import segmentation_models_pytorch as smp\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e64c3f",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff7d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(BaseDataset):\n",
    "    \"\"\"CamVid Dataset. Read images, apply augmentation and preprocessing transformations.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): path to images folder\n",
    "        masks_dir (str): path to segmentation masks folder\n",
    "        class_values (list): values of classes to extract from segmentation mask\n",
    "        augmentation (albumentations.Compose): data transfromation pipeline \n",
    "            (e.g. flip, scale, etc.)\n",
    "        preprocessing (albumentations.Compose): data preprocessing \n",
    "            (e.g. noralization, shape manipulation, etc.)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    CLASSES = ['bg', 'stas']\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir,\n",
    "            classes=None,\n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.ids = os.listdir(images_dir)\n",
    "        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n",
    "        \n",
    "        # convert str names to class values on masks\n",
    "        self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "        \n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        # read data\n",
    "        image = cv2.imread(self.images_fps[i])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image.shape != (1024, 512, 3):\n",
    "            image = cv2.resize(image, (1024, 512), interpolation=cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image)\n",
    "            image = sample['image']\n",
    "            \n",
    "        return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as albu\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing():\n",
    "    \"\"\"Construct preprocessing transform\n",
    "    \n",
    "    Args:\n",
    "        preprocessing_fn (callbale): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    _transform = [\n",
    "        albu.Lambda(image=to_tensor, mask=to_tensor),\n",
    "    ]\n",
    "    return albu.Compose(_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ff07c3",
   "metadata": {},
   "source": [
    "## Public Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5346a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = ['stas']\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "public_test = TestDataset(\n",
    "    './Data/Public_Image',\n",
    "    preprocessing=get_preprocessing(),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "public_test_dataloader = DataLoader(public_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af50d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_test_vis = TestDataset(\n",
    "    './Data/Public_Image',\n",
    "    classes=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f28b074",
   "metadata": {},
   "source": [
    "## Private Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a9370",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test = TestDataset(\n",
    "    './Data/Image',\n",
    "    preprocessing=get_preprocessing(),\n",
    "    classes=CLASSES,\n",
    ")\n",
    "\n",
    "private_test_dataloader = DataLoader(private_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f71ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "private_test_vis = TestDataset(\n",
    "    './Data/Image',\n",
    "    classes=CLASSES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path='./Best/Image'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b520ba",
   "metadata": {},
   "source": [
    "# Load weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68918735",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_1 = torch.load('./model_weight/best_model_1.pth')\n",
    "best_model_2 = torch.load('./model_weight/best_model_2.pth')\n",
    "best_model_3 = torch.load('./model_weight/best_model_3.pth')\n",
    "best_model_4 = torch.load('./model_weight/best_model_4.pth')\n",
    "best_model_5 = torch.load('./model_weight/best_model_5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0001f7da",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3bb8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(public_test)):\n",
    "    name = os.path.basename(public_test_vis.images_fps[i])\n",
    "    print(name)\n",
    "    image_vis = public_test_vis[0].astype('uint8')\n",
    "    image = public_test[i]\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    print(x_tensor.shape)\n",
    "    pr_mask_1 = best_model_1.predict(x_tensor)\n",
    "    pr_mask_2 = best_model_2.predict(x_tensor)\n",
    "    pr_mask_3 = best_model_3.predict(x_tensor)\n",
    "    pr_mask_4 = best_model_4.predict(x_tensor)\n",
    "    pr_mask_5 = best_model_5.predict(x_tensor)\n",
    "    preds = (pr_mask_1 + pr_mask_2 + pr_mask_3 + pr_mask_4 + pr_mask_5) / 5.\n",
    "    pr_mask = (preds[:, 0, ...] > 0.5)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "    len(pr_mask)\n",
    "    zeros = np.zeros((512, 1024))\n",
    "    zeros[...] = pr_mask * 255\n",
    "    zeros = cv2.resize(zeros, (1716, 942))\n",
    "    print(i)\n",
    "\n",
    "    image = cv2.resize(image[0], (1716, 942))\n",
    "\n",
    "    plt.figure(figsize=(24,8))\n",
    "    f, axarr = plt.subplots(2, figsize=(15,8))\n",
    "    axarr[0].imshow(image, cmap='gray')\n",
    "    axarr[1].imshow(zeros, cmap='gray')\n",
    "    #plt.imshow(zeros, cmap='gray')\n",
    "    plt.show()\n",
    "    print(os.path.join(out_path, name.replace('.jpg','.png')))\n",
    "    plt.imsave(os.path.join(out_path, name.replace('.jpg','.png')), zeros, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f513d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(private_test)):\n",
    "    #if i == 10:break\n",
    "    name = os.path.basename(private_test_vis.images_fps[i])\n",
    "    print(name)\n",
    "    image_vis = private_test_vis[0].astype('uint8')\n",
    "    image = private_test[i]\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    print(x_tensor.shape)\n",
    "    pr_mask_1 = best_model_1.predict(x_tensor)\n",
    "    pr_mask_2 = best_model_2.predict(x_tensor)\n",
    "    pr_mask_3 = best_model_3.predict(x_tensor)\n",
    "    pr_mask_4 = best_model_4.predict(x_tensor)\n",
    "    pr_mask_5 = best_model_5.predict(x_tensor)\n",
    "    preds = (pr_mask_1 + pr_mask_2 + pr_mask_3 + pr_mask_4 + pr_mask_5) / 5.\n",
    "    pr_mask = (preds[:, 0, ...] > 0.5)\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "    len(pr_mask)\n",
    "    zeros = np.zeros((512, 1024))\n",
    "    zeros[...] = pr_mask * 255\n",
    "    zeros = cv2.resize(zeros, (1716, 942))\n",
    "    print(i)\n",
    "\n",
    "    image = cv2.resize(image[0], (1716, 942))\n",
    "\n",
    "    plt.figure(figsize=(24,8))\n",
    "    f, axarr = plt.subplots(2, figsize=(15,8))\n",
    "    axarr[0].imshow(image, cmap='gray')\n",
    "    axarr[1].imshow(zeros, cmap='gray')\n",
    "    #plt.imshow(zeros, cmap='gray')\n",
    "    plt.show()\n",
    "    print(os.path.join(out_path, name.replace('.jpg','.png')))\n",
    "    plt.imsave(os.path.join(out_path, name.replace('.jpg','.png')), zeros, cmap='gray')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55415e5",
   "metadata": {},
   "source": [
    "# Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "imaPath = './Best/Image'\n",
    "output = './Best/Image_Postprocess'\n",
    "imaList = os.listdir(imaPath)\n",
    "\n",
    "for files in imaList:\n",
    "    path_ima = os.path.join(imaPath, files)\n",
    "    path_processed = os.path.join(output, files)\n",
    "    img = cv2.imread (path_ima, 0)\n",
    "    img = cv2.blur(img, (7, 7))\n",
    "    mask = np.zeros_like(img)\n",
    "    #print(np.shape (img))\n",
    "\n",
    "    ret, img = cv2.threshold(img, 127, 255, cv2.THRESH_BINARY)\n",
    "    _, contours,_= cv2.findContours(img, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    n = len(contours)\n",
    "    cv_contours = []\n",
    "    for contour in contours:\n",
    "        area = cv2.contourArea(contour)\n",
    "        if area <= 30000:\n",
    "            cv_contours.append(contour)\n",
    "        else:\n",
    "            continue\n",
    "    cv2.fillPoly(img, cv_contours, (255, 255, 255))\n",
    "    cv2.imwrite(path_processed, img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
